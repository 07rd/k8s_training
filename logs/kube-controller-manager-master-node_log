I0625 10:02:32.628686       1 serving.go:319] Generated self-signed cert in-memory
I0625 10:02:33.319145       1 controllermanager.go:164] Version: v1.15.0
I0625 10:02:33.320470       1 secure_serving.go:116] Serving securely on 127.0.0.1:10257
I0625 10:02:33.321367       1 deprecated_insecure_serving.go:53] Serving insecurely on [::]:10252
I0625 10:02:33.321706       1 leaderelection.go:235] attempting to acquire leader lease  kube-system/kube-controller-manager...
E0625 10:02:36.642013       1 leaderelection.go:324] error retrieving resource lock kube-system/kube-controller-manager: endpoints "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "endpoints" in API group "" in the namespace "kube-system"
I0625 10:02:39.039830       1 leaderelection.go:245] successfully acquired lease kube-system/kube-controller-manager
I0625 10:02:39.040791       1 event.go:258] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"0ccf771c-e6e5-4100-8b0a-a4f513cafe9f", APIVersion:"v1", ResourceVersion:"125", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' master-node_1388ec03-7303-4e1f-9259-e28f5630baa0 became leader
I0625 10:02:40.265558       1 plugins.go:103] No cloud provider specified.
I0625 10:02:40.266456       1 controller_utils.go:1029] Waiting for caches to sync for tokens controller
I0625 10:02:40.366885       1 controller_utils.go:1036] Caches are synced for tokens controller
I0625 10:02:40.405258       1 controllermanager.go:532] Started "daemonset"
I0625 10:02:40.405655       1 daemon_controller.go:267] Starting daemon sets controller
I0625 10:02:40.405691       1 controller_utils.go:1029] Waiting for caches to sync for daemon sets controller
I0625 10:02:40.442281       1 controllermanager.go:532] Started "statefulset"
W0625 10:02:40.442306       1 controllermanager.go:524] Skipping "ttl-after-finished"
W0625 10:02:40.442311       1 controllermanager.go:524] Skipping "root-ca-cert-publisher"
I0625 10:02:40.442585       1 stateful_set.go:145] Starting stateful set controller
I0625 10:02:40.442618       1 controller_utils.go:1029] Waiting for caches to sync for stateful set controller
I0625 10:02:40.475644       1 controllermanager.go:532] Started "endpoint"
I0625 10:02:40.475846       1 endpoints_controller.go:166] Starting endpoint controller
I0625 10:02:40.475967       1 controller_utils.go:1029] Waiting for caches to sync for endpoint controller
I0625 10:02:40.524568       1 controllermanager.go:532] Started "job"
I0625 10:02:40.524677       1 job_controller.go:143] Starting job controller
I0625 10:02:40.524718       1 controller_utils.go:1029] Waiting for caches to sync for job controller
I0625 10:02:40.554527       1 controllermanager.go:532] Started "clusterrole-aggregation"
I0625 10:02:40.554691       1 clusterroleaggregation_controller.go:148] Starting ClusterRoleAggregator
I0625 10:02:40.554711       1 controller_utils.go:1029] Waiting for caches to sync for ClusterRoleAggregator controller
I0625 10:02:40.574792       1 node_lifecycle_controller.go:77] Sending events to api server
E0625 10:02:40.574933       1 core.go:160] failed to start cloud node lifecycle controller: no cloud provider provided
W0625 10:02:40.574955       1 controllermanager.go:524] Skipping "cloud-node-lifecycle"
I0625 10:02:40.592841       1 controllermanager.go:532] Started "persistentvolume-binder"
I0625 10:02:40.593036       1 pv_controller_base.go:282] Starting persistent volume controller
I0625 10:02:40.593503       1 controller_utils.go:1029] Waiting for caches to sync for persistent volume controller
I0625 10:02:41.218505       1 controllermanager.go:532] Started "horizontalpodautoscaling"
I0625 10:02:41.218971       1 horizontal.go:156] Starting HPA controller
I0625 10:02:41.219210       1 controller_utils.go:1029] Waiting for caches to sync for HPA controller
I0625 10:02:41.468423       1 controllermanager.go:532] Started "csrcleaner"
I0625 10:02:41.468488       1 cleaner.go:81] Starting CSR cleaner controller
I0625 10:02:41.720043       1 controllermanager.go:532] Started "ttl"
W0625 10:02:41.720096       1 core.go:174] configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes.
W0625 10:02:41.720106       1 controllermanager.go:524] Skipping "route"
I0625 10:02:41.720145       1 ttl_controller.go:116] Starting TTL controller
I0625 10:02:41.720549       1 controller_utils.go:1029] Waiting for caches to sync for TTL controller
I0625 10:02:41.972238       1 controllermanager.go:532] Started "attachdetach"
I0625 10:02:41.973058       1 attach_detach_controller.go:335] Starting attach detach controller
I0625 10:02:41.973100       1 controller_utils.go:1029] Waiting for caches to sync for attach detach controller
I0625 10:02:42.220308       1 controllermanager.go:532] Started "serviceaccount"
I0625 10:02:42.220350       1 serviceaccounts_controller.go:117] Starting service account controller
I0625 10:02:42.220575       1 controller_utils.go:1029] Waiting for caches to sync for service account controller
I0625 10:02:43.069976       1 controllermanager.go:532] Started "garbagecollector"
I0625 10:02:43.071013       1 garbagecollector.go:128] Starting garbage collector controller
I0625 10:02:43.071044       1 controller_utils.go:1029] Waiting for caches to sync for garbage collector controller
I0625 10:02:43.071057       1 graph_builder.go:280] GraphBuilder running
I0625 10:02:43.129655       1 controllermanager.go:532] Started "cronjob"
I0625 10:02:43.130012       1 cronjob_controller.go:96] Starting CronJob Manager
I0625 10:02:43.156117       1 controllermanager.go:532] Started "bootstrapsigner"
I0625 10:02:43.156241       1 controller_utils.go:1029] Waiting for caches to sync for bootstrap_signer controller
E0625 10:02:43.369140       1 core.go:76] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail
W0625 10:02:43.369165       1 controllermanager.go:524] Skipping "service"
I0625 10:02:43.621868       1 controllermanager.go:532] Started "replicationcontroller"
I0625 10:02:43.621931       1 replica_set.go:182] Starting replicationcontroller controller
I0625 10:02:43.621951       1 controller_utils.go:1029] Waiting for caches to sync for ReplicationController controller
I0625 10:02:43.877639       1 controllermanager.go:532] Started "podgc"
I0625 10:02:43.877877       1 gc_controller.go:76] Starting GC controller
I0625 10:02:43.878879       1 controller_utils.go:1029] Waiting for caches to sync for GC controller
I0625 10:02:44.147029       1 controllermanager.go:532] Started "namespace"
I0625 10:02:44.147079       1 namespace_controller.go:186] Starting namespace controller
I0625 10:02:44.147097       1 controller_utils.go:1029] Waiting for caches to sync for namespace controller
I0625 10:02:44.527023       1 controllermanager.go:532] Started "disruption"
I0625 10:02:44.527425       1 disruption.go:333] Starting disruption controller
I0625 10:02:44.527487       1 controller_utils.go:1029] Waiting for caches to sync for disruption controller
I0625 10:02:44.671012       1 node_ipam_controller.go:94] Sending events to api server.
I0625 10:02:54.677867       1 range_allocator.go:78] Sending events to api server.
I0625 10:02:54.678027       1 range_allocator.go:99] No Service CIDR provided. Skipping filtering out service addresses.
I0625 10:02:54.678040       1 range_allocator.go:105] Node master-node has no CIDR, ignoring
I0625 10:02:54.678066       1 controllermanager.go:532] Started "nodeipam"
I0625 10:02:54.678185       1 node_ipam_controller.go:162] Starting ipam controller
I0625 10:02:54.678203       1 controller_utils.go:1029] Waiting for caches to sync for node controller
I0625 10:02:54.914523       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for replicasets.extensions
I0625 10:02:54.914912       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for statefulsets.apps
I0625 10:02:54.915303       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for serviceaccounts
I0625 10:02:54.915414       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for jobs.batch
I0625 10:02:54.915577       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
W0625 10:02:54.915621       1 shared_informer.go:364] resyncPeriod 52359408772092 is smaller than resyncCheckPeriod 63474182612518 and the informer has already started. Changing it to 63474182612518
I0625 10:02:54.915769       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for daemonsets.apps
I0625 10:02:54.916072       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
I0625 10:02:54.916370       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for cronjobs.batch
I0625 10:02:54.916456       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
I0625 10:02:54.916637       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for replicasets.apps
I0625 10:02:54.916686       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for controllerrevisions.apps
I0625 10:02:54.916741       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
I0625 10:02:54.916905       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for daemonsets.extensions
I0625 10:02:54.917562       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for ingresses.extensions
I0625 10:02:54.917771       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
I0625 10:02:54.917868       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for podtemplates
I0625 10:02:54.918058       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for deployments.extensions
I0625 10:02:54.918143       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for events.events.k8s.io
I0625 10:02:54.918195       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for ingresses.networking.k8s.io
I0625 10:02:54.918376       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for limitranges
I0625 10:02:54.918445       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for endpoints
I0625 10:02:54.918535       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkpolicies.extensions
I0625 10:02:54.918584       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for deployments.apps
I0625 10:02:54.918659       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
I0625 10:02:54.918688       1 controllermanager.go:532] Started "resourcequota"
I0625 10:02:54.919096       1 resource_quota_controller.go:271] Starting resource quota controller
I0625 10:02:54.919775       1 controller_utils.go:1029] Waiting for caches to sync for resource quota controller
I0625 10:02:54.920013       1 resource_quota_monitor.go:303] QuotaMonitor running
I0625 10:02:54.980151       1 controllermanager.go:532] Started "deployment"
I0625 10:02:54.980437       1 deployment_controller.go:152] Starting deployment controller
I0625 10:02:54.981822       1 controller_utils.go:1029] Waiting for caches to sync for deployment controller
I0625 10:02:54.990616       1 controllermanager.go:532] Started "csrapproving"
I0625 10:02:54.990739       1 certificate_controller.go:113] Starting certificate controller
I0625 10:02:54.990765       1 controller_utils.go:1029] Waiting for caches to sync for certificate controller
I0625 10:02:55.020540       1 controllermanager.go:532] Started "pvc-protection"
I0625 10:02:55.020655       1 pvc_protection_controller.go:100] Starting PVC protection controller
I0625 10:02:55.021485       1 controller_utils.go:1029] Waiting for caches to sync for PVC protection controller
I0625 10:02:55.042646       1 controllermanager.go:532] Started "pv-protection"
I0625 10:02:55.042849       1 pv_protection_controller.go:82] Starting PV protection controller
I0625 10:02:55.042874       1 controller_utils.go:1029] Waiting for caches to sync for PV protection controller
I0625 10:02:55.060811       1 controllermanager.go:532] Started "replicaset"
I0625 10:02:55.061729       1 replica_set.go:182] Starting replicaset controller
I0625 10:02:55.061761       1 controller_utils.go:1029] Waiting for caches to sync for ReplicaSet controller
E0625 10:02:55.071497       1 prometheus.go:55] failed to register depth metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071537       1 prometheus.go:68] failed to register adds metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071566       1 prometheus.go:82] failed to register latency metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071585       1 prometheus.go:96] failed to register workDuration metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071620       1 prometheus.go:112] failed to register unfinished metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071639       1 prometheus.go:126] failed to register unfinished metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071653       1 prometheus.go:152] failed to register depth metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071667       1 prometheus.go:164] failed to register adds metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071778       1 prometheus.go:176] failed to register latency metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071931       1 prometheus.go:188] failed to register work_duration metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071967       1 prometheus.go:203] failed to register unfinished_work_seconds metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.071987       1 prometheus.go:216] failed to register longest_running_processor_microseconds metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.072024       1 prometheus.go:139] failed to register retries metric certificate: duplicate metrics collector registration attempted
E0625 10:02:55.072084       1 prometheus.go:228] failed to register retries metric certificate: duplicate metrics collector registration attempted
I0625 10:02:55.072123       1 controllermanager.go:532] Started "csrsigning"
I0625 10:02:55.072479       1 certificate_controller.go:113] Starting certificate controller
I0625 10:02:55.072499       1 controller_utils.go:1029] Waiting for caches to sync for certificate controller
I0625 10:02:55.099185       1 controllermanager.go:532] Started "tokencleaner"
I0625 10:02:55.099325       1 tokencleaner.go:116] Starting token cleaner controller
I0625 10:02:55.099348       1 controller_utils.go:1029] Waiting for caches to sync for token_cleaner controller
I0625 10:02:55.133656       1 node_lifecycle_controller.go:290] Sending events to api server.
I0625 10:02:55.134192       1 node_lifecycle_controller.go:323] Controller is using taint based evictions.
I0625 10:02:55.134262       1 taint_manager.go:175] Sending events to api server.
I0625 10:02:55.135266       1 node_lifecycle_controller.go:388] Controller will reconcile labels.
I0625 10:02:55.135391       1 node_lifecycle_controller.go:401] Controller will taint node by condition.
I0625 10:02:55.135437       1 controllermanager.go:532] Started "nodelifecycle"
I0625 10:02:55.135501       1 node_lifecycle_controller.go:425] Starting node controller
I0625 10:02:55.135563       1 controller_utils.go:1029] Waiting for caches to sync for taint controller
I0625 10:02:55.199623       1 controller_utils.go:1036] Caches are synced for token_cleaner controller
I0625 10:02:55.383229       1 controllermanager.go:532] Started "persistentvolume-expander"
I0625 10:02:55.388438       1 expand_controller.go:300] Starting expand controller
I0625 10:02:55.388503       1 controller_utils.go:1029] Waiting for caches to sync for expand controller
I0625 10:02:55.395578       1 controller_utils.go:1029] Waiting for caches to sync for garbage collector controller
I0625 10:02:55.452503       1 controller_utils.go:1036] Caches are synced for PV protection controller
I0625 10:02:55.452836       1 controller_utils.go:1036] Caches are synced for namespace controller
I0625 10:02:55.465556       1 controller_utils.go:1036] Caches are synced for ClusterRoleAggregator controller
I0625 10:02:55.474632       1 controller_utils.go:1036] Caches are synced for certificate controller
I0625 10:02:55.491078       1 controller_utils.go:1036] Caches are synced for certificate controller
I0625 10:02:55.521083       1 controller_utils.go:1036] Caches are synced for service account controller
I0625 10:02:55.572791       1 log.go:172] [INFO] signed certificate with serial number 113687768695420790366521141194588510834051943658
I0625 10:02:55.665124       1 controller_utils.go:1036] Caches are synced for ReplicaSet controller
I0625 10:02:55.680074       1 controller_utils.go:1036] Caches are synced for GC controller
I0625 10:02:55.682786       1 controller_utils.go:1036] Caches are synced for deployment controller
I0625 10:02:55.689261       1 event.go:258] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"coredns", UID:"c27d3841-957b-4d7e-bc5d-721306d57d6d", APIVersion:"apps/v1", ResourceVersion:"205", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set coredns-5c98db65d4 to 2
I0625 10:02:55.710898       1 event.go:258] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"coredns-5c98db65d4", UID:"5443f017-e774-4515-a537-5f90e584ad26", APIVersion:"apps/v1", ResourceVersion:"329", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: coredns-5c98db65d4-7m2t7
I0625 10:02:55.722098       1 controller_utils.go:1036] Caches are synced for ReplicationController controller
I0625 10:02:55.729130       1 controller_utils.go:1036] Caches are synced for job controller
I0625 10:02:55.776352       1 controller_utils.go:1036] Caches are synced for endpoint controller
I0625 10:02:55.789020       1 event.go:258] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"coredns-5c98db65d4", UID:"5443f017-e774-4515-a537-5f90e584ad26", APIVersion:"apps/v1", ResourceVersion:"329", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: coredns-5c98db65d4-zrrhj
I0625 10:02:55.820072       1 controller_utils.go:1036] Caches are synced for HPA controller
I0625 10:02:55.928212       1 controller_utils.go:1036] Caches are synced for disruption controller
I0625 10:02:55.928414       1 disruption.go:341] Sending events to api server.
I0625 10:02:55.959858       1 controller_utils.go:1036] Caches are synced for bootstrap_signer controller
W0625 10:02:56.047028       1 actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="master-node" does not exist
I0625 10:02:56.083022       1 controller_utils.go:1036] Caches are synced for node controller
I0625 10:02:56.083155       1 range_allocator.go:157] Starting range CIDR allocator
I0625 10:02:56.083190       1 controller_utils.go:1029] Waiting for caches to sync for cidrallocator controller
I0625 10:02:56.120807       1 controller_utils.go:1036] Caches are synced for TTL controller
I0625 10:02:56.121628       1 controller_utils.go:1036] Caches are synced for PVC protection controller
I0625 10:02:56.145240       1 controller_utils.go:1036] Caches are synced for stateful set controller
I0625 10:02:56.175071       1 controller_utils.go:1036] Caches are synced for attach detach controller
I0625 10:02:56.186695       1 controller_utils.go:1036] Caches are synced for cidrallocator controller
I0625 10:02:56.189293       1 controller_utils.go:1036] Caches are synced for expand controller
I0625 10:02:56.194486       1 range_allocator.go:310] Set node master-node PodCIDR to 192.168.0.0/24
I0625 10:02:56.194698       1 controller_utils.go:1036] Caches are synced for persistent volume controller
I0625 10:02:56.196313       1 controller_utils.go:1036] Caches are synced for garbage collector controller
I0625 10:02:56.206045       1 controller_utils.go:1036] Caches are synced for daemon sets controller
I0625 10:02:56.219066       1 event.go:258] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-proxy", UID:"2ddcc7fa-8e13-4401-acc5-ee15844d4ac7", APIVersion:"apps/v1", ResourceVersion:"214", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-zxnww
I0625 10:02:56.221410       1 controller_utils.go:1036] Caches are synced for resource quota controller
I0625 10:02:56.236022       1 controller_utils.go:1036] Caches are synced for taint controller
I0625 10:02:56.236091       1 node_lifecycle_controller.go:1159] Initializing eviction metric for zone: 
I0625 10:02:56.236119       1 taint_manager.go:198] Starting NoExecuteTaintManager
W0625 10:02:56.236132       1 node_lifecycle_controller.go:833] Missing timestamp for Node master-node. Assuming now as a timestamp.
I0625 10:02:56.236164       1 node_lifecycle_controller.go:1009] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I0625 10:02:56.238027       1 event.go:258] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"master-node", UID:"a6c13996-e001-42f6-8d6d-a0b05c325a92", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node master-node event: Registered Node master-node in Controller
E0625 10:02:56.257323       1 daemon_controller.go:302] kube-system/kube-proxy failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy", UID:"2ddcc7fa-8e13-4401-acc5-ee15844d4ac7", ResourceVersion:"214", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697053761, loc:(*time.Location)(0x730bba0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc001a0acc0), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-proxy", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc001774780), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"xtables-lock", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc001a0ace0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"lib-modules", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc001a0ad00), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"kube-proxy", Image:"k8s.gcr.io/kube-proxy:v1.15.0", Command:[]string{"/usr/local/bin/kube-proxy", "--config=/var/lib/kube-proxy/config.conf", "--hostname-override=$(NODE_NAME)"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc001a0ad40)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-proxy", ReadOnly:false, MountPath:"/var/lib/kube-proxy", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"xtables-lock", ReadOnly:false, MountPath:"/run/xtables.lock", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"lib-modules", ReadOnly:true, MountPath:"/lib/modules", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc000912aa0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001766c38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"beta.kubernetes.io/os":"linux"}, ServiceAccountName:"kube-proxy", DeprecatedServiceAccount:"kube-proxy", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001783500), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"CriticalAddonsOnly", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"system-node-critical", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc0016c3678)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc001766c78)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kube-proxy": the object has been modified; please apply your changes to the latest version and try again
I0625 10:02:56.273215       1 controller_utils.go:1036] Caches are synced for garbage collector controller
I0625 10:02:56.273248       1 garbagecollector.go:137] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0625 10:02:56.611640       1 controller_utils.go:1029] Waiting for caches to sync for resource quota controller
I0625 10:02:56.714512       1 controller_utils.go:1036] Caches are synced for resource quota controller
I0625 10:07:06.002617       1 event.go:258] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-flannel-ds-amd64", UID:"7e57d465-112b-4459-8e7a-4c034b31fd54", APIVersion:"apps/v1", ResourceVersion:"687", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-flannel-ds-amd64-qrqb4
E0625 10:07:06.084255       1 daemon_controller.go:302] kube-system/kube-flannel-ds-amd64 failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-flannel-ds-amd64", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kube-flannel-ds-amd64", UID:"7e57d465-112b-4459-8e7a-4c034b31fd54", ResourceVersion:"687", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697054025, loc:(*time.Location)(0x730bba0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"flannel", "tier":"node"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1", "kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"DaemonSet\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"flannel\",\"tier\":\"node\"},\"name\":\"kube-flannel-ds-amd64\",\"namespace\":\"kube-system\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app\":\"flannel\",\"tier\":\"node\"}},\"spec\":{\"containers\":[{\"args\":[\"--ip-masq\",\"--kube-subnet-mgr\"],\"command\":[\"/opt/bin/flanneld\"],\"env\":[{\"name\":\"POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.name\"}}},{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\",\"name\":\"kube-flannel\",\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"50Mi\"}},\"securityContext\":{\"capabilities\":{\"add\":[\"NET_ADMIN\"]},\"privileged\":false},\"volumeMounts\":[{\"mountPath\":\"/run/flannel\",\"name\":\"run\"},{\"mountPath\":\"/etc/kube-flannel/\",\"name\":\"flannel-cfg\"}]}],\"hostNetwork\":true,\"initContainers\":[{\"args\":[\"-f\",\"/etc/kube-flannel/cni-conf.json\",\"/etc/cni/net.d/10-flannel.conflist\"],\"command\":[\"cp\"],\"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\",\"name\":\"install-cni\",\"volumeMounts\":[{\"mountPath\":\"/etc/cni/net.d\",\"name\":\"cni\"},{\"mountPath\":\"/etc/kube-flannel/\",\"name\":\"flannel-cfg\"}]}],\"nodeSelector\":{\"beta.kubernetes.io/arch\":\"amd64\"},\"serviceAccountName\":\"flannel\",\"tolerations\":[{\"effect\":\"NoSchedule\",\"operator\":\"Exists\"}],\"volumes\":[{\"hostPath\":{\"path\":\"/run/flannel\"},\"name\":\"run\"},{\"hostPath\":{\"path\":\"/etc/cni/net.d\"},\"name\":\"cni\"},{\"configMap\":{\"name\":\"kube-flannel-cfg\"},\"name\":\"flannel-cfg\"}]}}}}\n"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc000d88360), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"flannel", "tier":"node"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"run", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc000d88380), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"cni", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc000d883a0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"flannel-cfg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc001495100), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"install-cni", Image:"quay.io/coreos/flannel:v0.11.0-amd64", Command:[]string{"cp"}, Args:[]string{"-f", "/etc/kube-flannel/cni-conf.json", "/etc/cni/net.d/10-flannel.conflist"}, WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"cni", ReadOnly:false, MountPath:"/etc/cni/net.d", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"flannel-cfg", ReadOnly:false, MountPath:"/etc/kube-flannel/", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"kube-flannel", Image:"quay.io/coreos/flannel:v0.11.0-amd64", Command:[]string{"/opt/bin/flanneld"}, Args:[]string{"--ip-masq", "--kube-subnet-mgr"}, WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"POD_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc000d883e0)}, v1.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc000d88440)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"50Mi", Format:"BinarySI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"50Mi", Format:"BinarySI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"run", ReadOnly:false, MountPath:"/run/flannel", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"flannel-cfg", ReadOnly:false, MountPath:"/etc/kube-flannel/", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0020d10e0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0014d53c0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"beta.kubernetes.io/arch":"amd64"}, ServiceAccountName:"flannel", DeprecatedServiceAccount:"flannel", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000e9e6c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"OnDelete", RollingUpdate:(*v1.RollingUpdateDaemonSet)(nil)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc0014d5448)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kube-flannel-ds-amd64": the object has been modified; please apply your changes to the latest version and try again
I0625 10:07:21.440670       1 node_lifecycle_controller.go:1036] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I0625 10:09:28.162677       1 event.go:258] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"kubernetes-dashboard", UID:"0a11d129-82c5-4092-9dbb-5c2219e2455a", APIVersion:"apps/v1", ResourceVersion:"922", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set kubernetes-dashboard-7d75c474bb to 1
I0625 10:09:28.199498       1 event.go:258] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"kubernetes-dashboard-7d75c474bb", UID:"1819cb84-a361-404c-9f57-3029f9758df4", APIVersion:"apps/v1", ResourceVersion:"923", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kubernetes-dashboard-7d75c474bb-5s5th
W0625 10:18:43.514920       1 reflector.go:302] k8s.io/client-go/informers/factory.go:133: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0625 10:54:30.587337       1 reflector.go:302] k8s.io/client-go/informers/factory.go:133: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0625 11:18:03.723349       1 reflector.go:302] k8s.io/client-go/informers/factory.go:133: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0625 11:55:02.038148       1 reflector.go:302] k8s.io/client-go/informers/factory.go:133: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
